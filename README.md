### **Minho Ryu (bzantium)**

**Senior AI Research Engineer | Foundation Models & Scalable Systems | Google Developer Expert (AI)**

[![Linkedin Badge](https://img.shields.io/badge/-LinkedIn-blue?style=flat&logo=Linkedin&logoColor=white&link=https://www.linkedin.com/in/minhoryu/)](https://www.linkedin.com/in/minhoryu/) 
[![Gmail Badge](https://img.shields.io/badge/-Gmail-d14836?style=flat&logo=Gmail&logoColor=white&link=mailto:ryumin93@gmail.com)](mailto:ryumin93@gmail.com)
[![Google Scholar Badge](https://img.shields.io/badge/-Scholar-4285f4?style=flat&logo=google-scholar&logoColor=white&link=https://scholar.google.com/citations?hl=en&user=MHNVvuUAAAAJ)](https://scholar.google.com/citations?hl=en&user=MHNVvuUAAAAJ) 

### üëã About Me

I am a Senior AI Research Engineer and **Google Developer Expert (AI)** with a mission to architect and scale the powerful, efficient, and accessible large language models that will define the future.

My expertise covers the full lifecycle of foundation models: from curating massive datasets and architecting cutting-edge training infrastructure to developing production-grade models that set new performance benchmarks. I thrive on solving complex, large-scale challenges and am deeply invested in strengthening the open-source ecosystem that fuels global AI innovation.

### üöÄ Key Professional Highlights

  * **Foundation Model Development:** Co-led the end-to-end pre-training of Kakao's **Kanana V1** foundation model from a 3T token dataset and implemented compute-efficient scaling techniques like **Pruning & Distillation**. I also spearheaded key enhancements for **Kanana-1.5** (including its 128K long-context extension) and owned the full development of a production embedding model that surpassed larger competitors.
  * **Scalable AI Infrastructure:** Architected and optimized a cutting-edge, scalable LLM training pipeline from the ground up using **JAX, MaxText, and TPUs**. This work was featured in an official **[Google Cloud Blog Post](https://cloud.google.com/blog/products/infrastructure-modernization/kakaos-journey-with-jax-and-cloud-tpus?hl=en)** and my expertise was recognized with a presentation at **[Google Cloud Next 2025 (YouTube)](https://youtu.be/fcI9A-pjNlU?t=482)**.
  * **Open Source Leadership:** As a Research Lead at **EleutherAI**, I co-led the development of **[Polyglot-Ko](https://github.com/eleutherai/polyglot)**, the first open-source Korean large language model, successfully training and releasing models up to 12.8B parameters.

### üêô Open Source Contributions

My GitHub activity reflects a consistent track record of contributing high-impact code to the core of the modern AI ecosystem. I focus on strengthening foundational libraries, building scalable systems, and advancing rigorous evaluation. Below are some of my key contributions:

  * **Hugging Face Transformers:** Led the end-to-end integration of the [**DeepSeek-V3 model**](https://github.com/huggingface/transformers/pull/35926), a complex process that spanned its core architecture, unique RoPE scaling logic, and follow-on capabilities like [token classification heads](https://github.com/huggingface/transformers/pull/40641).
  * **Google's MaxText:** Architected and implemented the [**multi-source data blending feature**](https://github.com/AI-Hypercomputer/maxtext/pull/1801), significantly enhancing the data pipeline for Google's flagship JAX-based large-scale training framework.
  * **LM-Eval Harness:** Expanded the community's model evaluation capabilities by adding and integrating major benchmarks, including [**`global_mmlu`**](https://www.google.com/search?q=%5Bhttps://github.com/EleutherAI/lm-evaluation-harness/pull/2636%5D\(https://github.com/EleutherAI/lm-evaluation-harness/pull/2636\)), [**`hrm8k`**](https://www.google.com/search?q=%5Bhttps://github.com/EleutherAI/lm-evaluation-harness/pull/2627%5D\(https://github.com/EleutherAI/lm-evaluation-harness/pull/2627\)), and [**`humaneval+`**](https://www.google.com/search?q=%5Bhttps://github.com/EleutherAI/lm-evaluation-harness/pull/2734%5D\(https://github.com/EleutherAI/lm-evaluation-harness/pull/2734\)).
  * **Broader Ecosystem:** My contributions also span high-performance distributed training concepts like **Tensor Parallelism in EleutherAI's OSLO**, enabling [**distributed training for generative vision models**](https://github.com/lucidrains/muse-maskgit-pytorch/pull/19), and implementing [**new model support in LLM2Vec**](https://github.com/McGill-NLP/llm2vec/pull/117).

### ‚úçÔ∏è Publications

  * *Kanana: Compute-efficient Bilingual Language Models.* [[arXiv:2502.18934]](https://arxiv.org/abs/2502.18934)
  * *A Technical Report for Polyglot-Ko: Open-Source Large-Scale Korean Language Models.* [[arXiv:2306.02254]](https://arxiv.org/abs/2306.02254)
  * *Knowledge distillation for bert unsupervised domain adaptation.* [[arXiv:2010.11478]](https://arxiv.org/abs/2010.11478)
